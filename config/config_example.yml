### FILE LOCATIONS ###
metadata_paths: # paths to metadata files
  - "/path/to/data/metadata_0.csv"
  - "/path/to/data/metadata_1.csv"
lcdata_paths: # paths to observation data files
  - "~/path/to/data/lcdata_0.csv"
  - "~/path/to/data/lcdata_1.csv"
ids_path: "/path/to/output/dir/0_5_Ia_split_heatmaps_ids.hdf5" # path to ids file (output of data_cuts.py), will be in output_path
output_path: "/path/to/output/dir" # location of desired output directory for ids file + heatmaps

### HEATMAP SIZE PARAMS ###
# HEATMAP HEIGHT: wavelength_range / wavelength_interval. wavelength_range=7100 angstroms (3000 -> 10100), set in create_heatmaps.py
# HEATMAP WIDTH: mjd_range / mjd_interval. mjd_range=180 (peak_mjd-50 -> peak_mjd+130), set in create_heatmaps.py
num_wavelength_bins: 32
num_mjd_bins: 180

### CLASS BALANCING ###
# 0.5 for class-balanced binary split
# "categorical" for class-balanced categorical split
Ia_fraction: null 

### SPECIFIC TO CATEGORICAL SPLIT ###
# categorical_min_per_type: min number of SNe per type; types with fewer are discarded
# categorical_max_per_type: max number of SNe per type; this number of SNe will be randomly chosen if a type has more
categorical_min_per_type: 200
categorical_max_per_type: 2000

### SAVING INTERMEDIATE RESULTS IN DATA_CUTS.PY ###
save_to_json: True # save results of apply_cuts as json file (helpful if apply_cuts is time-intensive)
from_json: False # from_json: True will skip apply_cuts and instead read the results from json file (helpful if data_cuts.py died after apply_cuts but before finishing)

# mapping between numerical type ID and a name string
sn_type_id_to_name:
  42: "SNII"
  52: "SNIax"
  62: "SNIbc"
  67: "SNIa-91bg"
  64: "KN"
  90: "SNIa"
  95: "SLSN-1"

### MODEL ARCHITECTURE / TRAINING PARAMS ###
mode: "train" # or "predict", but have to specify path to trained model
trained_model: "/path/to/trained/model" # only needs to be specified in predict mode
class_balanced: True
categorical: False # categorical vs. binary classification
batch_size: 32
num_epochs: 400
train_proportion: 0.8
val_proportion: 0.1
has_ids: True
with_z: False # classification with/without redshift

### DEBUG AND LOGGING SETTINGS ###  # 30th Oct, 2025, A. Mitra - Debug flag and verbose logging controls
debug_flag: 0                      # Implementation selection: 0=refactored (default), -901=legacy (default: 0)
verbose_data_loading: False        # Enable detailed progress reporting (default: False). Can also use --verbose or -v CLI flag

### MEMORY OPTIMIZATION SETTINGS ###  # 8th Sept, 2025, A. Mitra - Comprehensive memory optimization section
# Basic memory optimization
memory_optimize: True              # Enable memory optimization features (default: True)
force_streaming: False             # Force streaming prediction regardless of size (default: False)
streaming_threshold: 75000         # Auto-streaming for datasets larger than this (default: 75000)
                                   # Note: This threshold adapts automatically based on dataset characteristics:
                                   #   - Large records (>10 MB each): threshold reduces to 5000-50000
                                   #   - Total size >200 GB: threshold reduces to 10000
                                   #   - Total size >100 GB: threshold reduces to 20000
                                   #   - Total size >40 GB: threshold reduces to 30000
gc_frequency: 50                   # Garbage collection frequency in batches (default: 50)

# Advanced memory optimization (NEW)
# NOTE: These settings are AUTO-CONFIGURED for large datasets (â‰¥75K samples or >40GB)  # 30th Oct, 2025, A. Mitra
# When auto-configured, uses: enable_micro_batching=true, micro_batch_size=16, chunk_size=400
# Only set explicitly if you want to override the automatic behavior
enable_dynamic_batch_size: True    # Enable adaptive batch sizing based on memory pressure (default: True)
enable_micro_batching: False       # Auto-enabled for large datasets (default: False, auto-set to true when needed)
micro_batch_size: 64               # Auto-set to 16 for large datasets (default: 64, auto-adjusted to 16 when needed)
chunk_size: 1000                   # Auto-set to 400 for large datasets (default: 1000, auto-adjusted to 400 when needed)

# Model optimization for inference
enable_model_quantization: False   # Enable model quantization for reduced memory usage (default: False)
quantization_method: "dynamic"     # Quantization method: "dynamic", "float16" (default: "dynamic")

# Disk caching and memory-mapped I/O
enable_disk_caching: False         # Enable disk-based caching for repeated runs (default: False)
disk_cache_dir: "/tmp/scone_cache" # Directory for disk cache (default: "/tmp/scone_cache")

# Ultra-low memory mode for extreme memory constraints (NEW)
ultra_low_memory_mode: False       # Enable maximum memory reduction strategies (default: False)
memory_target_gb: 50               # Target memory usage in GB - system will try to stay under this (default: 50)

# Debug and testing modes (NEW)
dry_run_mode: False                # Test baseline memory usage without loading data (default: False)
debug_pause_mode: False            # Enable pauses for memory inspection during execution (default: False)
pause_duration: 30                 # Duration of debug pauses in seconds (default: 30)
